2023-11-23 13:53:32,192 [INFO] Imported packages.
2023-11-23 13:53:32,193 [INFO] Training script started on 2023-11-23 13:53:32.192570.
2023-11-23 13:53:32,826 [INFO] Using device: cuda
2023-11-23 13:53:32,872 [INFO] Train dataset size: 47023
2023-11-23 13:53:32,873 [INFO] Validation dataset size: 5225
2023-11-23 13:53:32,895 [INFO] Using 2 GPUs!
2023-11-23 13:53:35,317 [INFO] Number of trainable parameters: 194979
2023-11-23 13:53:35,317 [INFO] Model initialized.
2023-11-23 13:53:35,318 [INFO] Scheduler initialized.
2023-11-23 13:53:53,598 [INFO] Epoch [1/1000], Batch [1/46], Loss: 2.0677
2023-11-23 13:54:02,435 [INFO] Epoch [1/1000], Batch [11/46], Loss: 1.6233
2023-11-23 13:54:11,132 [INFO] Epoch [1/1000], Batch [21/46], Loss: 1.4441
2023-11-23 13:54:19,828 [INFO] Epoch [1/1000], Batch [31/46], Loss: 1.4554
2023-11-23 13:54:28,228 [INFO] Epoch [1/1000], Batch [41/46], Loss: 1.4131
2023-11-23 13:55:15,341 [INFO] Epoch 1/1000, Train Loss: 1.5637932471607043, Val Loss: 1.3635387818018596
2023-11-23 13:55:24,227 [INFO] Epoch [2/1000], Batch [1/46], Loss: 1.3739
2023-11-23 13:55:32,559 [INFO] Epoch [2/1000], Batch [11/46], Loss: 1.3495
2023-11-23 13:55:41,167 [INFO] Epoch [2/1000], Batch [21/46], Loss: 1.3730
2023-11-23 13:55:50,737 [INFO] Epoch [2/1000], Batch [31/46], Loss: 1.3638
2023-11-23 13:56:00,763 [INFO] Epoch [2/1000], Batch [41/46], Loss: 1.3362
2023-11-23 13:56:47,229 [INFO] Epoch 2/1000, Train Loss: 1.3514167096303857, Val Loss: 1.3257672786712646
2023-11-23 13:56:55,974 [INFO] Epoch [3/1000], Batch [1/46], Loss: 1.3520
2023-11-23 13:57:04,606 [INFO] Epoch [3/1000], Batch [11/46], Loss: 1.3179
2023-11-23 13:57:13,261 [INFO] Epoch [3/1000], Batch [21/46], Loss: 1.3031
2023-11-23 13:57:21,585 [INFO] Epoch [3/1000], Batch [31/46], Loss: 1.3374
2023-11-23 13:57:30,081 [INFO] Epoch [3/1000], Batch [41/46], Loss: 1.3349
2023-11-23 13:58:14,224 [INFO] Epoch 3/1000, Train Loss: 1.328890955966452, Val Loss: 1.2959952751795452
2023-11-23 13:58:23,107 [INFO] Epoch [4/1000], Batch [1/46], Loss: 1.3074
2023-11-23 13:58:31,305 [INFO] Epoch [4/1000], Batch [11/46], Loss: 1.3199
2023-11-23 13:58:40,016 [INFO] Epoch [4/1000], Batch [21/46], Loss: 1.3205
2023-11-23 13:58:48,588 [INFO] Epoch [4/1000], Batch [31/46], Loss: 1.3326
2023-11-23 13:58:57,302 [INFO] Epoch [4/1000], Batch [41/46], Loss: 1.2756
2023-11-23 13:59:45,271 [INFO] Epoch 4/1000, Train Loss: 1.3098310024841973, Val Loss: 1.2882548967997234
2023-11-23 13:59:55,440 [INFO] Epoch [5/1000], Batch [1/46], Loss: 1.2779
2023-11-23 14:00:04,763 [INFO] Epoch [5/1000], Batch [11/46], Loss: 1.2816
2023-11-23 14:00:13,382 [INFO] Epoch [5/1000], Batch [21/46], Loss: 1.2918
2023-11-23 14:00:22,028 [INFO] Epoch [5/1000], Batch [31/46], Loss: 1.3242
2023-11-23 14:00:30,686 [INFO] Epoch [5/1000], Batch [41/46], Loss: 1.2885
2023-11-23 14:01:17,508 [INFO] Epoch 5/1000, Train Loss: 1.2891969577125881, Val Loss: 1.2724075118700664
2023-11-23 14:01:28,281 [INFO] Epoch [6/1000], Batch [1/46], Loss: 1.3143
2023-11-23 14:01:37,964 [INFO] Epoch [6/1000], Batch [11/46], Loss: 1.3055
2023-11-23 14:01:47,043 [INFO] Epoch [6/1000], Batch [21/46], Loss: 1.3130
2023-11-23 14:01:55,755 [INFO] Epoch [6/1000], Batch [31/46], Loss: 1.2956
2023-11-23 14:02:04,744 [INFO] Epoch [6/1000], Batch [41/46], Loss: 1.2931
2023-11-23 14:02:51,709 [INFO] Epoch 6/1000, Train Loss: 1.286918098511903, Val Loss: 1.2783613602320354
2023-11-23 14:03:00,750 [INFO] Epoch [7/1000], Batch [1/46], Loss: 1.2869
2023-11-23 14:03:09,049 [INFO] Epoch [7/1000], Batch [11/46], Loss: 1.2365
2023-11-23 14:03:17,691 [INFO] Epoch [7/1000], Batch [21/46], Loss: 1.2827
2023-11-23 14:03:26,310 [INFO] Epoch [7/1000], Batch [31/46], Loss: 1.2786
2023-11-23 14:03:34,979 [INFO] Epoch [7/1000], Batch [41/46], Loss: 1.3406
2023-11-23 14:04:24,541 [INFO] Epoch 7/1000, Train Loss: 1.2822735205940579, Val Loss: 1.257089336713155
2023-11-23 14:04:33,821 [INFO] Epoch [8/1000], Batch [1/46], Loss: 1.2695
2023-11-23 14:04:42,769 [INFO] Epoch [8/1000], Batch [11/46], Loss: 1.2704
2023-11-23 14:04:52,100 [INFO] Epoch [8/1000], Batch [21/46], Loss: 1.2946
2023-11-23 14:05:00,859 [INFO] Epoch [8/1000], Batch [31/46], Loss: 1.2841
2023-11-23 14:05:09,620 [INFO] Epoch [8/1000], Batch [41/46], Loss: 1.2657
2023-11-23 14:05:57,841 [INFO] Epoch 8/1000, Train Loss: 1.2818912811901257, Val Loss: 1.2573015292485554
2023-11-23 14:06:07,393 [INFO] Epoch [9/1000], Batch [1/46], Loss: 1.2529
2023-11-23 14:06:16,107 [INFO] Epoch [9/1000], Batch [11/46], Loss: 1.3020
2023-11-23 14:06:24,941 [INFO] Epoch [9/1000], Batch [21/46], Loss: 1.2599
2023-11-23 14:06:33,652 [INFO] Epoch [9/1000], Batch [31/46], Loss: 1.2560
2023-11-23 14:06:42,885 [INFO] Epoch [9/1000], Batch [41/46], Loss: 1.2779
2023-11-23 14:07:32,428 [INFO] Epoch 9/1000, Train Loss: 1.2822299444157144, Val Loss: 1.258697748184204
2023-11-23 14:07:41,799 [INFO] Epoch [10/1000], Batch [1/46], Loss: 1.2782
2023-11-23 14:07:50,709 [INFO] Epoch [10/1000], Batch [11/46], Loss: 1.2690
2023-11-23 14:07:59,918 [INFO] Epoch [10/1000], Batch [21/46], Loss: 1.3208
2023-11-23 14:08:08,930 [INFO] Epoch [10/1000], Batch [31/46], Loss: 1.2475
2023-11-23 14:08:17,805 [INFO] Epoch [10/1000], Batch [41/46], Loss: 1.2885
2023-11-23 14:09:09,028 [INFO] Epoch 10/1000, Train Loss: 1.2797364618467248, Val Loss: 1.2675368189811707
2023-11-23 14:09:18,840 [INFO] Epoch [11/1000], Batch [1/46], Loss: 1.2794
2023-11-23 14:09:26,771 [INFO] Epoch [11/1000], Batch [11/46], Loss: 1.2372
2023-11-23 14:09:34,515 [INFO] Epoch [11/1000], Batch [21/46], Loss: 1.2954
2023-11-23 14:09:42,112 [INFO] Epoch [11/1000], Batch [31/46], Loss: 1.2618
2023-11-23 14:09:50,928 [INFO] Epoch [11/1000], Batch [41/46], Loss: 1.2765
2023-11-23 14:10:38,468 [INFO] Epoch 11/1000, Train Loss: 1.280740033025327, Val Loss: 1.23653644323349
2023-11-23 14:10:47,369 [INFO] Epoch [12/1000], Batch [1/46], Loss: 1.2859
2023-11-23 14:10:55,893 [INFO] Epoch [12/1000], Batch [11/46], Loss: 1.2956
2023-11-23 14:11:04,388 [INFO] Epoch [12/1000], Batch [21/46], Loss: 1.2702
2023-11-23 14:11:13,033 [INFO] Epoch [12/1000], Batch [31/46], Loss: 1.3150
2023-11-23 14:11:21,645 [INFO] Epoch [12/1000], Batch [41/46], Loss: 1.2809
2023-11-23 14:12:10,883 [INFO] Epoch 12/1000, Train Loss: 1.2790633673253267, Val Loss: 1.2586879928906758
2023-11-23 14:12:20,563 [INFO] Epoch [13/1000], Batch [1/46], Loss: 1.2594
2023-11-23 14:12:29,488 [INFO] Epoch [13/1000], Batch [11/46], Loss: 1.2740
2023-11-23 14:12:37,508 [INFO] Epoch [13/1000], Batch [21/46], Loss: 1.3106
2023-11-23 14:12:45,975 [INFO] Epoch [13/1000], Batch [31/46], Loss: 1.2474
2023-11-23 14:12:54,601 [INFO] Epoch [13/1000], Batch [41/46], Loss: 1.2727
2023-11-23 14:13:43,618 [INFO] Epoch 13/1000, Train Loss: 1.2785765943319902, Val Loss: 1.2665226062138875
2023-11-23 14:13:52,829 [INFO] Epoch [14/1000], Batch [1/46], Loss: 1.3010
2023-11-23 14:14:01,205 [INFO] Epoch [14/1000], Batch [11/46], Loss: 1.2524
2023-11-23 14:14:09,284 [INFO] Epoch [14/1000], Batch [21/46], Loss: 1.3021
2023-11-23 14:14:17,714 [INFO] Epoch [14/1000], Batch [31/46], Loss: 1.2744
2023-11-23 14:14:25,812 [INFO] Epoch [14/1000], Batch [41/46], Loss: 1.3082
2023-11-23 14:15:13,438 [INFO] Epoch 14/1000, Train Loss: 1.2784075788829639, Val Loss: 1.2677985827128093
2023-11-23 14:15:22,631 [INFO] Epoch [15/1000], Batch [1/46], Loss: 1.2716
2023-11-23 14:15:33,051 [INFO] Epoch [15/1000], Batch [11/46], Loss: 1.3032
2023-11-23 14:15:43,117 [INFO] Epoch [15/1000], Batch [21/46], Loss: 1.3039
2023-11-23 14:15:53,458 [INFO] Epoch [15/1000], Batch [31/46], Loss: 1.3122
2023-11-23 14:16:02,854 [INFO] Epoch [15/1000], Batch [41/46], Loss: 1.2679
2023-11-23 14:16:50,548 [INFO] Learning rate changed from 0.001 to 0.0001
2023-11-23 14:16:50,549 [INFO] Epoch 15/1000, Train Loss: 1.277858246927676, Val Loss: 1.2737726767857869
2023-11-23 14:17:00,016 [INFO] Epoch [16/1000], Batch [1/46], Loss: 1.2695
2023-11-23 14:17:10,157 [INFO] Epoch [16/1000], Batch [11/46], Loss: 1.2721
2023-11-23 14:17:20,392 [INFO] Epoch [16/1000], Batch [21/46], Loss: 1.3023
2023-11-23 14:17:30,437 [INFO] Epoch [16/1000], Batch [31/46], Loss: 1.2616
2023-11-23 14:17:40,705 [INFO] Epoch [16/1000], Batch [41/46], Loss: 1.2810
2023-11-23 14:18:29,009 [INFO] Epoch 16/1000, Train Loss: 1.2780011425847593, Val Loss: 1.2600411772727966
2023-11-23 14:18:38,885 [INFO] Epoch [17/1000], Batch [1/46], Loss: 1.2473
2023-11-23 14:18:49,429 [INFO] Epoch [17/1000], Batch [11/46], Loss: 1.2260
2023-11-23 14:18:59,368 [INFO] Epoch [17/1000], Batch [21/46], Loss: 1.2856
2023-11-23 14:19:09,900 [INFO] Epoch [17/1000], Batch [31/46], Loss: 1.3098
2023-11-23 14:19:20,034 [INFO] Epoch [17/1000], Batch [41/46], Loss: 1.2601
2023-11-23 14:20:08,496 [INFO] Epoch 17/1000, Train Loss: 1.2771238347758418, Val Loss: 1.244687835375468
2023-11-23 14:20:17,956 [INFO] Epoch [18/1000], Batch [1/46], Loss: 1.2496
2023-11-23 14:20:27,865 [INFO] Epoch [18/1000], Batch [11/46], Loss: 1.2383
2023-11-23 14:20:38,154 [INFO] Epoch [18/1000], Batch [21/46], Loss: 1.2873
2023-11-23 14:20:48,761 [INFO] Epoch [18/1000], Batch [31/46], Loss: 1.2627
2023-11-23 14:20:59,225 [INFO] Epoch [18/1000], Batch [41/46], Loss: 1.2939
2023-11-23 14:21:47,309 [INFO] Epoch 18/1000, Train Loss: 1.2776778288509534, Val Loss: 1.2706958651542664
2023-11-23 14:21:56,471 [INFO] Epoch [19/1000], Batch [1/46], Loss: 1.2559
2023-11-23 14:22:06,701 [INFO] Epoch [19/1000], Batch [11/46], Loss: 1.2648
2023-11-23 14:22:16,577 [INFO] Epoch [19/1000], Batch [21/46], Loss: 1.2921
2023-11-23 14:22:27,189 [INFO] Epoch [19/1000], Batch [31/46], Loss: 1.2775
2023-11-23 14:22:36,706 [INFO] Epoch [19/1000], Batch [41/46], Loss: 1.2844
2023-11-23 14:23:25,133 [INFO] Learning rate changed from 0.0001 to 1e-05
2023-11-23 14:23:25,134 [INFO] Epoch 19/1000, Train Loss: 1.2777065505152163, Val Loss: 1.2454895774523418
2023-11-23 14:23:34,535 [INFO] Epoch [20/1000], Batch [1/46], Loss: 1.2495
2023-11-23 14:23:45,246 [INFO] Epoch [20/1000], Batch [11/46], Loss: 1.2791
2023-11-23 14:23:55,786 [INFO] Epoch [20/1000], Batch [21/46], Loss: 1.2703
2023-11-23 14:24:06,065 [INFO] Epoch [20/1000], Batch [31/46], Loss: 1.2981
2023-11-23 14:24:15,974 [INFO] Epoch [20/1000], Batch [41/46], Loss: 1.3062
2023-11-23 14:25:04,457 [INFO] Epoch 20/1000, Train Loss: 1.2777930109397224, Val Loss: 1.266942024230957
2023-11-23 14:25:13,708 [INFO] Epoch [21/1000], Batch [1/46], Loss: 1.2439
2023-11-23 14:25:24,438 [INFO] Epoch [21/1000], Batch [11/46], Loss: 1.2487
2023-11-23 14:25:34,587 [INFO] Epoch [21/1000], Batch [21/46], Loss: 1.2841
2023-11-23 14:25:44,595 [INFO] Epoch [21/1000], Batch [31/46], Loss: 1.2454
2023-11-23 14:25:54,302 [INFO] Epoch [21/1000], Batch [41/46], Loss: 1.2848
2023-11-23 14:26:41,639 [INFO] Epoch 21/1000, Train Loss: 1.2767094658768696, Val Loss: 1.2668432195981343
2023-11-23 14:26:50,650 [INFO] Epoch [22/1000], Batch [1/46], Loss: 1.2689
2023-11-23 14:27:00,765 [INFO] Epoch [22/1000], Batch [11/46], Loss: 1.2334
2023-11-23 14:27:11,039 [INFO] Epoch [22/1000], Batch [21/46], Loss: 1.2423
2023-11-23 14:27:21,619 [INFO] Epoch [22/1000], Batch [31/46], Loss: 1.2871
2023-11-23 14:27:31,659 [INFO] Epoch [22/1000], Batch [41/46], Loss: 1.3183
2023-11-23 14:28:19,115 [INFO] Epoch 22/1000, Train Loss: 1.2770343189654143, Val Loss: 1.264206071694692
2023-11-23 14:28:28,074 [INFO] Epoch [23/1000], Batch [1/46], Loss: 1.2975
2023-11-23 14:28:38,166 [INFO] Epoch [23/1000], Batch [11/46], Loss: 1.2884
2023-11-23 14:28:48,468 [INFO] Epoch [23/1000], Batch [21/46], Loss: 1.2886
2023-11-23 14:28:58,641 [INFO] Epoch [23/1000], Batch [31/46], Loss: 1.3100
2023-11-23 14:29:08,513 [INFO] Epoch [23/1000], Batch [41/46], Loss: 1.3171
2023-11-23 14:29:56,882 [INFO] Learning rate reached minimum value 1.0000000000000002e-06, stopping training!

2023-11-22 23:55:28,233 [INFO] Imported packages.
2023-11-22 23:55:28,233 [INFO] Training script started on 2023-11-22 23:55:28.232570.
2023-11-22 23:55:28,891 [INFO] Using device: cuda
2023-11-22 23:55:28,926 [INFO] Train dataset size: 41798
2023-11-22 23:55:28,926 [INFO] Validation dataset size: 10450
2023-11-22 23:55:28,951 [INFO] Using 2 GPUs!
2023-11-22 23:55:32,558 [INFO] Number of trainable parameters: 194979
2023-11-22 23:55:32,558 [INFO] Model initialized.
2023-11-22 23:55:32,559 [INFO] Scheduler initialized.
2023-11-22 23:55:50,278 [INFO] Epoch [1/1000], Batch [1/164], Loss: 2.2441
2023-11-22 23:55:53,030 [INFO] Epoch [1/1000], Batch [11/164], Loss: 1.8617
2023-11-22 23:55:56,082 [INFO] Epoch [1/1000], Batch [21/164], Loss: 1.4689
2023-11-22 23:56:00,725 [INFO] Epoch [1/1000], Batch [31/164], Loss: 1.4129
2023-11-22 23:56:05,623 [INFO] Epoch [1/1000], Batch [41/164], Loss: 1.3653
2023-11-22 23:56:10,756 [INFO] Epoch [1/1000], Batch [51/164], Loss: 1.3474
2023-11-22 23:56:17,099 [INFO] Epoch [1/1000], Batch [61/164], Loss: 1.3227
2023-11-22 23:56:21,451 [INFO] Epoch [1/1000], Batch [71/164], Loss: 1.3403
2023-11-22 23:56:27,715 [INFO] Epoch [1/1000], Batch [81/164], Loss: 1.3774
2023-11-22 23:56:32,568 [INFO] Epoch [1/1000], Batch [91/164], Loss: 1.3228
2023-11-22 23:56:37,371 [INFO] Epoch [1/1000], Batch [101/164], Loss: 1.2956
2023-11-22 23:56:41,940 [INFO] Epoch [1/1000], Batch [111/164], Loss: 1.2642
2023-11-22 23:56:48,389 [INFO] Epoch [1/1000], Batch [121/164], Loss: 1.2781
2023-11-22 23:56:52,972 [INFO] Epoch [1/1000], Batch [131/164], Loss: 1.3405
2023-11-22 23:56:57,498 [INFO] Epoch [1/1000], Batch [141/164], Loss: 1.3250
2023-11-22 23:57:01,955 [INFO] Epoch [1/1000], Batch [151/164], Loss: 1.3190
2023-11-22 23:57:08,186 [INFO] Epoch [1/1000], Batch [161/164], Loss: 1.2357
2023-11-22 23:58:30,201 [INFO] Epoch 1/1000, Train Loss: 1.4102063506114773, Val Loss: 1.3085804974160544
2023-11-22 23:58:36,317 [INFO] Epoch [2/1000], Batch [1/164], Loss: 1.3010
2023-11-22 23:58:41,499 [INFO] Epoch [2/1000], Batch [11/164], Loss: 1.3042
2023-11-22 23:58:46,548 [INFO] Epoch [2/1000], Batch [21/164], Loss: 1.3175
2023-11-22 23:58:51,353 [INFO] Epoch [2/1000], Batch [31/164], Loss: 1.2883
2023-11-22 23:58:57,805 [INFO] Epoch [2/1000], Batch [41/164], Loss: 1.3268
2023-11-22 23:59:02,499 [INFO] Epoch [2/1000], Batch [51/164], Loss: 1.3478
2023-11-22 23:59:07,141 [INFO] Epoch [2/1000], Batch [61/164], Loss: 1.3348
2023-11-22 23:59:11,823 [INFO] Epoch [2/1000], Batch [71/164], Loss: 1.2416
2023-11-22 23:59:17,996 [INFO] Epoch [2/1000], Batch [81/164], Loss: 1.2120
2023-11-22 23:59:23,543 [INFO] Epoch [2/1000], Batch [91/164], Loss: 1.2521
2023-11-22 23:59:28,257 [INFO] Epoch [2/1000], Batch [101/164], Loss: 1.3027
2023-11-22 23:59:33,360 [INFO] Epoch [2/1000], Batch [111/164], Loss: 1.2789
2023-11-22 23:59:38,261 [INFO] Epoch [2/1000], Batch [121/164], Loss: 1.3024
2023-11-22 23:59:44,428 [INFO] Epoch [2/1000], Batch [131/164], Loss: 1.2720
2023-11-22 23:59:49,053 [INFO] Epoch [2/1000], Batch [141/164], Loss: 1.2771
2023-11-22 23:59:53,336 [INFO] Epoch [2/1000], Batch [151/164], Loss: 1.2559
2023-11-22 23:59:58,311 [INFO] Epoch [2/1000], Batch [161/164], Loss: 1.2981
2023-11-23 00:01:23,286 [INFO] Epoch 2/1000, Train Loss: 1.300013697728878, Val Loss: 1.2887959480285645
2023-11-23 00:01:28,792 [INFO] Epoch [3/1000], Batch [1/164], Loss: 1.2510
2023-11-23 00:01:33,547 [INFO] Epoch [3/1000], Batch [11/164], Loss: 1.2838
2023-11-23 00:01:38,286 [INFO] Epoch [3/1000], Batch [21/164], Loss: 1.3527
2023-11-23 00:01:43,013 [INFO] Epoch [3/1000], Batch [31/164], Loss: 1.2429
2023-11-23 00:01:48,199 [INFO] Epoch [3/1000], Batch [41/164], Loss: 1.2701
2023-11-23 00:01:54,457 [INFO] Epoch [3/1000], Batch [51/164], Loss: 1.3101
2023-11-23 00:01:59,207 [INFO] Epoch [3/1000], Batch [61/164], Loss: 1.3196
2023-11-23 00:02:03,889 [INFO] Epoch [3/1000], Batch [71/164], Loss: 1.4270
2023-11-23 00:02:08,498 [INFO] Epoch [3/1000], Batch [81/164], Loss: 1.2856
2023-11-23 00:02:15,290 [INFO] Epoch [3/1000], Batch [91/164], Loss: 1.3508
2023-11-23 00:02:19,799 [INFO] Epoch [3/1000], Batch [101/164], Loss: 1.3409
2023-11-23 00:02:24,246 [INFO] Epoch [3/1000], Batch [111/164], Loss: 1.3250
2023-11-23 00:02:28,839 [INFO] Epoch [3/1000], Batch [121/164], Loss: 1.1535
2023-11-23 00:02:34,954 [INFO] Epoch [3/1000], Batch [131/164], Loss: 1.2376
2023-11-23 00:02:40,465 [INFO] Epoch [3/1000], Batch [141/164], Loss: 1.3233
2023-11-23 00:02:45,212 [INFO] Epoch [3/1000], Batch [151/164], Loss: 1.3300
2023-11-23 00:02:49,612 [INFO] Epoch [3/1000], Batch [161/164], Loss: 1.2138
2023-11-23 00:04:12,449 [INFO] Epoch 3/1000, Train Loss: 1.2902926540956265, Val Loss: 1.2823100671535586
2023-11-23 00:04:17,906 [INFO] Epoch [4/1000], Batch [1/164], Loss: 1.2416
2023-11-23 00:04:22,513 [INFO] Epoch [4/1000], Batch [11/164], Loss: 1.2906
2023-11-23 00:04:26,919 [INFO] Epoch [4/1000], Batch [21/164], Loss: 1.3534
2023-11-23 00:04:31,775 [INFO] Epoch [4/1000], Batch [31/164], Loss: 1.3317
2023-11-23 00:04:37,626 [INFO] Epoch [4/1000], Batch [41/164], Loss: 1.2290
2023-11-23 00:04:43,100 [INFO] Epoch [4/1000], Batch [51/164], Loss: 1.2850
2023-11-23 00:04:47,696 [INFO] Epoch [4/1000], Batch [61/164], Loss: 1.2511
2023-11-23 00:04:52,105 [INFO] Epoch [4/1000], Batch [71/164], Loss: 1.2501
2023-11-23 00:04:56,879 [INFO] Epoch [4/1000], Batch [81/164], Loss: 1.3067
2023-11-23 00:05:03,543 [INFO] Epoch [4/1000], Batch [91/164], Loss: 1.3359
2023-11-23 00:05:08,230 [INFO] Epoch [4/1000], Batch [101/164], Loss: 1.3176
2023-11-23 00:05:13,060 [INFO] Epoch [4/1000], Batch [111/164], Loss: 1.3055
2023-11-23 00:05:17,872 [INFO] Epoch [4/1000], Batch [121/164], Loss: 1.2803
2023-11-23 00:05:24,805 [INFO] Epoch [4/1000], Batch [131/164], Loss: 1.2760
2023-11-23 00:05:29,689 [INFO] Epoch [4/1000], Batch [141/164], Loss: 1.3361
2023-11-23 00:05:34,223 [INFO] Epoch [4/1000], Batch [151/164], Loss: 1.2570
2023-11-23 00:05:38,545 [INFO] Epoch [4/1000], Batch [161/164], Loss: 1.2671
2023-11-23 00:07:05,521 [INFO] Epoch 4/1000, Train Loss: 1.2875888245861704, Val Loss: 1.2826975554954716
2023-11-23 00:07:11,551 [INFO] Epoch [5/1000], Batch [1/164], Loss: 1.2622
2023-11-23 00:07:16,534 [INFO] Epoch [5/1000], Batch [11/164], Loss: 1.3236
2023-11-23 00:07:21,364 [INFO] Epoch [5/1000], Batch [21/164], Loss: 1.3669
2023-11-23 00:07:25,931 [INFO] Epoch [5/1000], Batch [31/164], Loss: 1.2849
2023-11-23 00:07:32,829 [INFO] Epoch [5/1000], Batch [41/164], Loss: 1.2753
2023-11-23 00:07:37,752 [INFO] Epoch [5/1000], Batch [51/164], Loss: 1.2875
2023-11-23 00:07:42,554 [INFO] Epoch [5/1000], Batch [61/164], Loss: 1.2443
2023-11-23 00:07:47,210 [INFO] Epoch [5/1000], Batch [71/164], Loss: 1.3508
2023-11-23 00:07:54,502 [INFO] Epoch [5/1000], Batch [81/164], Loss: 1.3490
2023-11-23 00:07:59,343 [INFO] Epoch [5/1000], Batch [91/164], Loss: 1.3274
2023-11-23 00:08:04,379 [INFO] Epoch [5/1000], Batch [101/164], Loss: 1.3010
2023-11-23 00:08:08,768 [INFO] Epoch [5/1000], Batch [111/164], Loss: 1.2775
2023-11-23 00:08:15,989 [INFO] Epoch [5/1000], Batch [121/164], Loss: 1.3299
2023-11-23 00:08:20,869 [INFO] Epoch [5/1000], Batch [131/164], Loss: 1.3490
2023-11-23 00:08:25,819 [INFO] Epoch [5/1000], Batch [141/164], Loss: 1.2594
2023-11-23 00:08:30,550 [INFO] Epoch [5/1000], Batch [151/164], Loss: 1.2919
2023-11-23 00:08:37,023 [INFO] Epoch [5/1000], Batch [161/164], Loss: 1.3758
2023-11-23 00:09:57,897 [INFO] Epoch 5/1000, Train Loss: 1.2863506667497682, Val Loss: 1.282189930357584
2023-11-23 00:10:03,554 [INFO] Epoch [6/1000], Batch [1/164], Loss: 1.2887
2023-11-23 00:10:08,279 [INFO] Epoch [6/1000], Batch [11/164], Loss: 1.3012
2023-11-23 00:10:12,997 [INFO] Epoch [6/1000], Batch [21/164], Loss: 1.3757
2023-11-23 00:10:17,746 [INFO] Epoch [6/1000], Batch [31/164], Loss: 1.2753
2023-11-23 00:10:22,770 [INFO] Epoch [6/1000], Batch [41/164], Loss: 1.2503
2023-11-23 00:10:29,337 [INFO] Epoch [6/1000], Batch [51/164], Loss: 1.2569
2023-11-23 00:10:34,054 [INFO] Epoch [6/1000], Batch [61/164], Loss: 1.2677
2023-11-23 00:10:38,851 [INFO] Epoch [6/1000], Batch [71/164], Loss: 1.3283
2023-11-23 00:10:43,474 [INFO] Epoch [6/1000], Batch [81/164], Loss: 1.2954
2023-11-23 00:10:50,386 [INFO] Epoch [6/1000], Batch [91/164], Loss: 1.2244
2023-11-23 00:10:55,122 [INFO] Epoch [6/1000], Batch [101/164], Loss: 1.2569
2023-11-23 00:10:59,718 [INFO] Epoch [6/1000], Batch [111/164], Loss: 1.2931
2023-11-23 00:11:04,519 [INFO] Epoch [6/1000], Batch [121/164], Loss: 1.2446
2023-11-23 00:11:11,349 [INFO] Epoch [6/1000], Batch [131/164], Loss: 1.3222
2023-11-23 00:11:16,127 [INFO] Epoch [6/1000], Batch [141/164], Loss: 1.3278
2023-11-23 00:11:20,900 [INFO] Epoch [6/1000], Batch [151/164], Loss: 1.3804
2023-11-23 00:11:25,456 [INFO] Epoch [6/1000], Batch [161/164], Loss: 1.2591
2023-11-23 00:12:45,116 [INFO] Epoch 6/1000, Train Loss: 1.2863324037412318, Val Loss: 1.276429263556876
2023-11-23 00:12:50,817 [INFO] Epoch [7/1000], Batch [1/164], Loss: 1.3004
2023-11-23 00:12:55,399 [INFO] Epoch [7/1000], Batch [11/164], Loss: 1.3910
2023-11-23 00:12:59,948 [INFO] Epoch [7/1000], Batch [21/164], Loss: 1.2455
2023-11-23 00:13:04,812 [INFO] Epoch [7/1000], Batch [31/164], Loss: 1.3544
2023-11-23 00:13:10,057 [INFO] Epoch [7/1000], Batch [41/164], Loss: 1.2734
2023-11-23 00:13:15,472 [INFO] Epoch [7/1000], Batch [51/164], Loss: 1.2408
2023-11-23 00:13:21,222 [INFO] Epoch [7/1000], Batch [61/164], Loss: 1.2063
2023-11-23 00:13:25,926 [INFO] Epoch [7/1000], Batch [71/164], Loss: 1.2704
2023-11-23 00:13:30,848 [INFO] Epoch [7/1000], Batch [81/164], Loss: 1.1926
2023-11-23 00:13:35,579 [INFO] Epoch [7/1000], Batch [91/164], Loss: 1.3347
2023-11-23 00:13:42,988 [INFO] Epoch [7/1000], Batch [101/164], Loss: 1.3117
2023-11-23 00:13:47,698 [INFO] Epoch [7/1000], Batch [111/164], Loss: 1.4176
2023-11-23 00:13:52,318 [INFO] Epoch [7/1000], Batch [121/164], Loss: 1.2582
2023-11-23 00:13:56,950 [INFO] Epoch [7/1000], Batch [131/164], Loss: 1.3288
2023-11-23 00:14:03,300 [INFO] Epoch [7/1000], Batch [141/164], Loss: 1.2478
2023-11-23 00:14:08,231 [INFO] Epoch [7/1000], Batch [151/164], Loss: 1.3390
2023-11-23 00:14:13,218 [INFO] Epoch [7/1000], Batch [161/164], Loss: 1.2969
2023-11-23 00:15:32,675 [INFO] Epoch 7/1000, Train Loss: 1.2845935072840713, Val Loss: 1.2790264792558623
2023-11-23 00:15:38,109 [INFO] Epoch [8/1000], Batch [1/164], Loss: 1.3451
2023-11-23 00:15:42,744 [INFO] Epoch [8/1000], Batch [11/164], Loss: 1.2248
2023-11-23 00:15:47,473 [INFO] Epoch [8/1000], Batch [21/164], Loss: 1.2909
2023-11-23 00:15:52,172 [INFO] Epoch [8/1000], Batch [31/164], Loss: 1.3274
2023-11-23 00:15:57,181 [INFO] Epoch [8/1000], Batch [41/164], Loss: 1.3844
2023-11-23 00:16:03,621 [INFO] Epoch [8/1000], Batch [51/164], Loss: 1.2547
2023-11-23 00:16:08,300 [INFO] Epoch [8/1000], Batch [61/164], Loss: 1.2317
2023-11-23 00:16:13,001 [INFO] Epoch [8/1000], Batch [71/164], Loss: 1.2673
2023-11-23 00:16:17,657 [INFO] Epoch [8/1000], Batch [81/164], Loss: 1.2265
2023-11-23 00:16:24,650 [INFO] Epoch [8/1000], Batch [91/164], Loss: 1.3099
2023-11-23 00:16:29,305 [INFO] Epoch [8/1000], Batch [101/164], Loss: 1.3664
2023-11-23 00:16:34,048 [INFO] Epoch [8/1000], Batch [111/164], Loss: 1.2424
2023-11-23 00:16:38,609 [INFO] Epoch [8/1000], Batch [121/164], Loss: 1.2438
2023-11-23 00:16:44,763 [INFO] Epoch [8/1000], Batch [131/164], Loss: 1.2862
2023-11-23 00:16:49,106 [INFO] Epoch [8/1000], Batch [141/164], Loss: 1.2589
2023-11-23 00:16:54,584 [INFO] Epoch [8/1000], Batch [151/164], Loss: 1.3891
2023-11-23 00:16:59,518 [INFO] Epoch [8/1000], Batch [161/164], Loss: 1.2906
2023-11-23 00:18:16,894 [INFO] Epoch 8/1000, Train Loss: 1.2843419756831191, Val Loss: 1.2780392460706758
2023-11-23 00:18:22,440 [INFO] Epoch [9/1000], Batch [1/164], Loss: 1.3563
2023-11-23 00:18:27,268 [INFO] Epoch [9/1000], Batch [11/164], Loss: 1.2277
2023-11-23 00:18:31,998 [INFO] Epoch [9/1000], Batch [21/164], Loss: 1.3227
2023-11-23 00:18:36,653 [INFO] Epoch [9/1000], Batch [31/164], Loss: 1.2789
2023-11-23 00:18:42,301 [INFO] Epoch [9/1000], Batch [41/164], Loss: 1.3105
2023-11-23 00:18:48,281 [INFO] Epoch [9/1000], Batch [51/164], Loss: 1.2105
2023-11-23 00:18:53,006 [INFO] Epoch [9/1000], Batch [61/164], Loss: 1.3713
2023-11-23 00:18:57,645 [INFO] Epoch [9/1000], Batch [71/164], Loss: 1.2624
2023-11-23 00:19:02,432 [INFO] Epoch [9/1000], Batch [81/164], Loss: 1.3020
2023-11-23 00:19:09,300 [INFO] Epoch [9/1000], Batch [91/164], Loss: 1.2973
2023-11-23 00:19:14,047 [INFO] Epoch [9/1000], Batch [101/164], Loss: 1.2731
2023-11-23 00:19:18,802 [INFO] Epoch [9/1000], Batch [111/164], Loss: 1.2651
2023-11-23 00:19:23,481 [INFO] Epoch [9/1000], Batch [121/164], Loss: 1.2709
2023-11-23 00:19:30,462 [INFO] Epoch [9/1000], Batch [131/164], Loss: 1.2919
2023-11-23 00:19:35,117 [INFO] Epoch [9/1000], Batch [141/164], Loss: 1.2558
2023-11-23 00:19:39,868 [INFO] Epoch [9/1000], Batch [151/164], Loss: 1.2686
2023-11-23 00:19:44,450 [INFO] Epoch [9/1000], Batch [161/164], Loss: 1.2515
2023-11-23 00:21:04,187 [INFO] Early stopping triggered after 9 epochs!

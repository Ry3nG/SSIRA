2023-11-23 11:33:26,115 [INFO] Imported packages.
2023-11-23 11:33:26,116 [INFO] Training script started on 2023-11-23 11:33:26.115622.
2023-11-23 11:33:26,877 [INFO] Using device: cuda
2023-11-23 11:33:26,910 [INFO] Train dataset size: 47023
2023-11-23 11:33:26,910 [INFO] Validation dataset size: 5225
2023-11-23 11:33:26,931 [INFO] Using 2 GPUs!
2023-11-23 11:33:28,170 [INFO] Number of trainable parameters: 194979
2023-11-23 11:33:28,171 [INFO] Model initialized.
2023-11-23 11:33:28,171 [INFO] Scheduler initialized.
2023-11-23 11:33:51,805 [INFO] Epoch [1/1000], Batch [1/23], Loss: 2.1197
2023-11-23 11:34:03,935 [INFO] Epoch [1/1000], Batch [11/23], Loss: 1.6638
2023-11-23 11:34:20,809 [INFO] Epoch [1/1000], Batch [21/23], Loss: 1.4242
2023-11-23 11:35:14,135 [INFO] Epoch 1/1000, Train Loss: 1.719422495883444, Val Loss: 1.4419854879379272
2023-11-23 11:35:31,812 [INFO] Epoch [2/1000], Batch [1/23], Loss: 1.4542
2023-11-23 11:35:49,097 [INFO] Epoch [2/1000], Batch [11/23], Loss: 1.4156
2023-11-23 11:36:05,779 [INFO] Epoch [2/1000], Batch [21/23], Loss: 1.3533
2023-11-23 11:36:55,123 [INFO] Epoch 2/1000, Train Loss: 1.4150906282922495, Val Loss: 1.3833481868108113
2023-11-23 11:37:13,042 [INFO] Epoch [3/1000], Batch [1/23], Loss: 1.3528
2023-11-23 11:37:29,914 [INFO] Epoch [3/1000], Batch [11/23], Loss: 1.3596
2023-11-23 11:37:45,521 [INFO] Epoch [3/1000], Batch [21/23], Loss: 1.3423
2023-11-23 11:38:39,148 [INFO] Epoch 3/1000, Train Loss: 1.3529353452765422, Val Loss: 1.3488723834355671
2023-11-23 11:38:56,901 [INFO] Epoch [4/1000], Batch [1/23], Loss: 1.3397
2023-11-23 11:39:13,317 [INFO] Epoch [4/1000], Batch [11/23], Loss: 1.3248
2023-11-23 11:39:29,172 [INFO] Epoch [4/1000], Batch [21/23], Loss: 1.3727
2023-11-23 11:40:24,700 [INFO] Epoch 4/1000, Train Loss: 1.341994425524836, Val Loss: 1.3369999726613362
2023-11-23 11:40:41,140 [INFO] Epoch [5/1000], Batch [1/23], Loss: 1.3204
2023-11-23 11:40:58,511 [INFO] Epoch [5/1000], Batch [11/23], Loss: 1.3378
2023-11-23 11:41:15,243 [INFO] Epoch [5/1000], Batch [21/23], Loss: 1.3289
2023-11-23 11:42:05,877 [INFO] Epoch 5/1000, Train Loss: 1.3373414278030396, Val Loss: 1.3367748260498047
2023-11-23 11:42:23,249 [INFO] Epoch [6/1000], Batch [1/23], Loss: 1.3430
2023-11-23 11:42:40,296 [INFO] Epoch [6/1000], Batch [11/23], Loss: 1.3375
2023-11-23 11:42:56,434 [INFO] Epoch [6/1000], Batch [21/23], Loss: 1.3507
2023-11-23 11:43:47,692 [INFO] Epoch 6/1000, Train Loss: 1.335079120553058, Val Loss: 1.3265715042750041
2023-11-23 11:44:05,211 [INFO] Epoch [7/1000], Batch [1/23], Loss: 1.3192
2023-11-23 11:44:22,022 [INFO] Epoch [7/1000], Batch [11/23], Loss: 1.3321
2023-11-23 11:44:38,832 [INFO] Epoch [7/1000], Batch [21/23], Loss: 1.3200
2023-11-23 11:45:28,714 [INFO] Epoch 7/1000, Train Loss: 1.3222178842710413, Val Loss: 1.320020079612732
2023-11-23 11:45:45,392 [INFO] Epoch [8/1000], Batch [1/23], Loss: 1.3139
2023-11-23 11:46:01,511 [INFO] Epoch [8/1000], Batch [11/23], Loss: 1.3269
2023-11-23 11:46:16,870 [INFO] Epoch [8/1000], Batch [21/23], Loss: 1.2862
2023-11-23 11:47:06,245 [INFO] Epoch 8/1000, Train Loss: 1.3165424803028936, Val Loss: 1.316131313641866
2023-11-23 11:47:22,879 [INFO] Epoch [9/1000], Batch [1/23], Loss: 1.3006
2023-11-23 11:47:39,755 [INFO] Epoch [9/1000], Batch [11/23], Loss: 1.3206
2023-11-23 11:47:56,816 [INFO] Epoch [9/1000], Batch [21/23], Loss: 1.3101
2023-11-23 11:48:46,604 [INFO] Epoch 9/1000, Train Loss: 1.314245934071748, Val Loss: 1.323705792427063
2023-11-23 11:49:03,466 [INFO] Epoch [10/1000], Batch [1/23], Loss: 1.3299
2023-11-23 11:49:20,932 [INFO] Epoch [10/1000], Batch [11/23], Loss: 1.3217
2023-11-23 11:49:37,261 [INFO] Epoch [10/1000], Batch [21/23], Loss: 1.3149
2023-11-23 11:50:26,692 [INFO] Epoch 10/1000, Train Loss: 1.3095867322838826, Val Loss: 1.3035591046015422
2023-11-23 11:50:44,784 [INFO] Epoch [11/1000], Batch [1/23], Loss: 1.3226
2023-11-23 11:51:02,894 [INFO] Epoch [11/1000], Batch [11/23], Loss: 1.3225
2023-11-23 11:51:19,032 [INFO] Epoch [11/1000], Batch [21/23], Loss: 1.3187
2023-11-23 11:52:11,132 [INFO] Epoch 11/1000, Train Loss: 1.3052412012349004, Val Loss: 1.299553116162618
2023-11-23 11:52:29,081 [INFO] Epoch [12/1000], Batch [1/23], Loss: 1.2903
2023-11-23 11:52:47,262 [INFO] Epoch [12/1000], Batch [11/23], Loss: 1.2831
2023-11-23 11:53:04,522 [INFO] Epoch [12/1000], Batch [21/23], Loss: 1.2963
2023-11-23 11:53:55,342 [INFO] Epoch 12/1000, Train Loss: 1.2976957663245823, Val Loss: 1.2963520288467407
2023-11-23 11:54:13,116 [INFO] Epoch [13/1000], Batch [1/23], Loss: 1.2745
2023-11-23 11:54:31,112 [INFO] Epoch [13/1000], Batch [11/23], Loss: 1.2866
2023-11-23 11:54:48,524 [INFO] Epoch [13/1000], Batch [21/23], Loss: 1.2743
2023-11-23 11:55:38,994 [INFO] Epoch 13/1000, Train Loss: 1.2887559662694517, Val Loss: 1.2920960585276287
2023-11-23 11:55:56,287 [INFO] Epoch [14/1000], Batch [1/23], Loss: 1.2807
2023-11-23 11:56:13,923 [INFO] Epoch [14/1000], Batch [11/23], Loss: 1.2741
2023-11-23 11:56:30,978 [INFO] Epoch [14/1000], Batch [21/23], Loss: 1.2861
2023-11-23 11:57:23,092 [INFO] Epoch 14/1000, Train Loss: 1.286148304524629, Val Loss: 1.2854921023050945
2023-11-23 11:57:40,138 [INFO] Epoch [15/1000], Batch [1/23], Loss: 1.2930
2023-11-23 11:58:00,691 [INFO] Epoch [15/1000], Batch [11/23], Loss: 1.2960
2023-11-23 11:58:18,645 [INFO] Epoch [15/1000], Batch [21/23], Loss: 1.2613
2023-11-23 11:59:12,149 [INFO] Epoch 15/1000, Train Loss: 1.2833797050559002, Val Loss: 1.2897842725118
2023-11-23 11:59:29,868 [INFO] Epoch [16/1000], Batch [1/23], Loss: 1.2517
2023-11-23 11:59:46,455 [INFO] Epoch [16/1000], Batch [11/23], Loss: 1.2661
2023-11-23 12:00:02,578 [INFO] Epoch [16/1000], Batch [21/23], Loss: 1.2614
2023-11-23 12:00:55,199 [INFO] Epoch 16/1000, Train Loss: 1.2853063137634941, Val Loss: 1.2881018718083699
2023-11-23 12:01:12,326 [INFO] Epoch [17/1000], Batch [1/23], Loss: 1.2787
2023-11-23 12:01:32,307 [INFO] Epoch [17/1000], Batch [11/23], Loss: 1.2669
2023-11-23 12:01:49,606 [INFO] Epoch [17/1000], Batch [21/23], Loss: 1.2972
2023-11-23 12:02:44,393 [INFO] Early stopping triggered after 17 epochs!
